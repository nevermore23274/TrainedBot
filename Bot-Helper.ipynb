{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5de112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, torch, csv, nltk, json\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c407f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d55374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c19b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91080118",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51987a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(10):  # let's pretend we have 10 batches\n",
    "        # create some dummy input data and target labels\n",
    "        inputs = torch.randn(1, 1, 32, 32)\n",
    "        labels = torch.tensor([1])\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.tensor([labels]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:    # print every 2 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344b5db",
   "metadata": {},
   "source": [
    "# Bot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parent directory\n",
    "parent_dir = \"cloned_repos\"\n",
    "\n",
    "# Create the parent directory if it doesn't exist\n",
    "os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "# Read the repository URLs from the file\n",
    "with open('training-repos.txt', 'r') as file:\n",
    "    repos = file.readlines()\n",
    "\n",
    "# Remove any trailing whitespace or newlines\n",
    "repos = [repo.strip() for repo in repos]\n",
    "\n",
    "# Clone each repository into the parent directory\n",
    "for repo_url in repos:\n",
    "    repo_name = repo_url.split('/')[-1].replace('.git', '')  # Extract the repository name\n",
    "    target_dir = os.path.join(parent_dir, repo_name)  # Define the target directory\n",
    "    subprocess.run(['git', 'clone', repo_url, target_dir])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the NLTK tokenizer is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a directory for the preprocessed data\n",
    "os.makedirs('preprocessed_data', exist_ok=True)\n",
    "\n",
    "# Function to preprocess and tokenize a PowerShell script\n",
    "def preprocess_script(script_content):\n",
    "    tokens = word_tokenize(script_content)\n",
    "    return tokens\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path, unique_sequences):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            script_content = f.read()\n",
    "        tokens = preprocess_script(script_content)\n",
    "        sequence_str = ' '.join(tokens)\n",
    "        if sequence_str not in unique_sequences:\n",
    "            unique_sequences.add(sequence_str)\n",
    "            # Save the preprocessed tokens to the 'preprocessed_data' directory\n",
    "            with open(f'preprocessed_data/{os.path.basename(file_path)}.txt', 'w') as out_file:\n",
    "                out_file.write(sequence_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Set to track unique sequences\n",
    "unique_sequences = set()\n",
    "\n",
    "# Use ThreadPoolExecutor to process files in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers as needed\n",
    "    for root, dirs, files in os.walk('cloned_repos'):\n",
    "        for file in files:\n",
    "            if file.endswith('.ps1'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                executor.submit(process_file, file_path, unique_sequences)\n",
    "\n",
    "# Load feedback data from CSV and preprocess it\n",
    "feedback_data = []\n",
    "with open('data/feedback_data.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # Preprocess user_correction column and tokenize\n",
    "        corrected_tokens = preprocess_script(row['user_correction'])\n",
    "        feedback_data.append(corrected_tokens)\n",
    "\n",
    "# Save the feedback data into the preprocessed_data folder\n",
    "for i, tokens in enumerate(feedback_data):\n",
    "    with open(f'preprocessed_data/feedback_{i}.txt', 'w') as out_file:\n",
    "        out_file.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4c0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf cloned_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f6cea",
   "metadata": {},
   "source": [
    "# Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81ea05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BotDataset(Dataset):\n",
    "    def __init__(self, data_dir, sequence_length=50):\n",
    "        self.data_dir = data_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.tokens = self.load_tokens()\n",
    "        self.vocab = self.build_vocab()\n",
    "        self.vocab['<UNK>'] = len(self.vocab)  # Ensure <UNK> token is added to the vocabulary\n",
    "        self.token_indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in self.tokens]\n",
    "        self.num_samples = max(0, len(self.token_indices) - sequence_length)\n",
    "\n",
    "    def load_tokens(self):\n",
    "        tokens = []\n",
    "        for file in os.listdir(self.data_dir):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                file_tokens = f.read().split()\n",
    "                print(f\"Read {len(file_tokens)} tokens from {file_path}\")  # Debugging print\n",
    "                tokens.extend(file_tokens)\n",
    "        print(f\"Total tokens loaded: {len(tokens)}\")  # Debugging print\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self):\n",
    "        counts = Counter(self.tokens)\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(counts.items())}\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.token_indices[idx : idx + self.sequence_length]),\n",
    "            torch.tensor(self.token_indices[idx + 1 : idx + self.sequence_length + 1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8953a4",
   "metadata": {},
   "source": [
    "# Define RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308a1405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "# Print a message to confirm that the class is defined\n",
    "print(\"RNNModel class defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476c1de",
   "metadata": {},
   "source": [
    "# Prepare Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BotDataset('preprocessed_data', sequence_length=50)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a2336",
   "metadata": {},
   "source": [
    "# Save Model Configuration to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e83ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the model's architecture\n",
    "config = {\n",
    "    \"vocab_size\": len(dataset.vocab) + 1,  # Add 1 for the <UNK> token\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_layers\": 2\n",
    "}\n",
    "\n",
    "# Save the configuration to a JSON file\n",
    "with open('model/config.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# Assuming dataset.vocab is your token-to-index mapping\n",
    "vocab = dataset.vocab  # Replace with your actual vocab object\n",
    "vocab['<UNK>'] = len(vocab)  # Add <UNK> token to the vocab\n",
    "\n",
    "vocab_inv = {index: token for token, index in vocab.items()}  # Create the index-to-token mapping\n",
    "\n",
    "# Save the vocab mapping\n",
    "with open('model/vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "# Save the vocab_inv mapping\n",
    "with open('model/vocab_inv.json', 'w') as f:\n",
    "    json.dump(vocab_inv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7671e8",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3064a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNModel(len(dataset.vocab) + 1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 200  # Increased number of epochs\n",
    "early_stopping_patience = 5  # How many epochs to wait after last time validation loss improved\n",
    "best_val_loss = np.inf  # Initialize with a very large number\n",
    "epochs_no_improve = 0  # Counter for epochs with no improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, len(dataset.vocab) + 1), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:  # Print every 100 batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i}, Loss: {loss.item()}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, len(dataset.vocab) + 1), targets.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'model/bot_rnn_model_best.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('model/bot_rnn_model_best.pth'))\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'model/bot_rnn_model_state_dict.pth')\n",
    "\n",
    "print('Model saved successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
